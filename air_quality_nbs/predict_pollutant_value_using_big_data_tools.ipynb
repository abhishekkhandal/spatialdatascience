{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis = GIS('https://ndhagsb01.esri.com/portal', \n",
    "          'admin', \n",
    "          'esri.agp2', \n",
    "          profile=\"your_enterprise_portal\", verify_cert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcgis.geoanalytics.is_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata_datastore_manager = arcgis.geoanalytics.get_datastores()\n",
    "bigdata_datastore_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item = bigdata_datastore_manager.add_bigdata(\"air_quality_2019\", r\"\\\\DELDEVD014\\store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata_fileshares = bigdata_datastore_manager.search()\n",
    "bigdata_fileshares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = gis.content.search(\"\", item_type = \"big data file share\", max_items=40)\n",
    "search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_item = search_result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_lyr = air_item.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = describe_dataset(input_layer=air_lyr,\n",
    "                               extent_output=True,\n",
    "                               sample_size=1000,\n",
    "                               output_name=\"Description of air quality 2019 data\" + str(dt.now().microsecond),\n",
    "                               return_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description.output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = description.sample_layer.query().sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processsing():\n",
    "    from datetime import datetime as dt\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    # Load the big data file share layer into a DataFrame.\n",
    "    df = layers[0]\n",
    "    cols = ['Site Num', 'County Code', 'State Code', 'Date Local', 'Time Local', 'Parameter Name', 'Sample Measurement']\n",
    "    df = df.select(cols)\n",
    "    df = df.withColumn('Site_Num', F.lpad(df['Site Num'], 4, '0'))\n",
    "    df = df.withColumn('County_Code', F.lpad(df['County Code'], 3, '0'))\n",
    "    df = df.withColumn('State_Code', F.lpad(df['State Code'], 2, '0'))\n",
    "    df = df.withColumn('unique_id', F.concat(F.col('State_Code'), F.col('County_Code'), F.col('Site_Num')))\n",
    "#     drop_cols = ['Site_Num', 'County_Code', 'State_Code', 'Site Num', 'County Code', 'State Code']\n",
    "    df = df.drop('Site_Num', 'County_Code', 'State_Code', 'Site Num', 'County Code', 'State Code')\n",
    "    df = df.withColumn('datetime', concat(col(\"Date Local\"), lit(\" \"), col(\"Time Local\")))\n",
    "#     drop_cols = ['Time Local', 'Date Local']\n",
    "    df = df.drop('Time Local', 'Date Local')\n",
    "    df = df.where(col(\"unique_id\") == df.first().unique_id)\n",
    "    # group the dataframe by TextType field and count the number of calls for each call type. \n",
    "    df = df.groupby(df['datetime'], df['unique_id']).pivot(\"Parameter Name\").avg(\"Sample Measurement\")\n",
    "\n",
    "    df.write.format(\"webgis\").save(\"timeseries_data\" + str(dt.now().microsecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(code=data_processsing, layers=[air_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    import os\n",
    "    from pyspark.ml.regression import RandomForestRegressor\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "    # The training dataset is a feature service with the revenue and demographics of current customer areas.\n",
    "    data = layers[0]\n",
    "    data = data.filter(data.PM2_5___Local_Conditions.isNotNull())\n",
    "\n",
    "    # Combine explanatory columns into a single column called \"features\"\n",
    "    assembler = VectorAssembler(inputCols=['Outdoor_Temperature', \n",
    "                                           'Relative_Humidity', \n",
    "                                           'Wind_Direction___Resultant', \n",
    "                                           'Wind_Speed___Resultant', \n",
    "                                           'PM10_Total_0_10um_STP'], \n",
    "                                outputCol='features')\n",
    "    data = assembler.setHandleInvalid(\"skip\").transform(data)\n",
    "\n",
    "    # Split the dataset to keep 10% for model validation\n",
    "    (trainingData, testData) = data.randomSplit([0.9, 0.1])\n",
    "\n",
    "    # Create the Random Forest model and fit it using the training data\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"PM2_5___Local_Conditions\", numTrees=100, seed=14389)\n",
    "    model = rf.fit(trainingData)\n",
    "\n",
    "    # Apply the model to the test data removed earlier for validation\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Calculate and print the Root Mean Squared Error between model results and actual revenue\n",
    "    evaluator = RegressionEvaluator(labelCol=\"PM2_5___Local_Conditions\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "    # Write a summary of feature importance to a shared folder\n",
    "    out_path = r\"\\\\DELDEVD014\\store\"\n",
    "    with open(os.path.join(out_path, 'feature_importance.txt'), 'w') as w:\n",
    "        w.write(str(model.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(code=predict, layers=[series_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = gis.content.get('10308a3e7dd7424592a1e3c648ac37b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = counties.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average():\n",
    "    from datetime import datetime as dt\n",
    "    df = layers[0]\n",
    "    df = df.filter(df['Parameter Name'] == 'PM2.5 - Local Conditions')\n",
    "    res = geoanalytics.join_features(target_layer=layers[1], \n",
    "                                     join_layer=df, \n",
    "                                     join_operation=\"JoinOneToOne\",\n",
    "                                     summary_fields=[{'statisticType' : 'mean', 'onStatisticField' : 'Sample Measurement'}],\n",
    "                                     spatial_relationship='Contains')\n",
    "    res.write.format(\"webgis\").save(\"average_pm_by_boundaryFinal\" + str(dt.now().microsecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(average, [air_lyr, boundary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_parse():\n",
    "    from datetime import datetime as dt\n",
    "    from pyspark.sql import functions as F\n",
    "#     from pyspark.sql.functions import concat, col, lit\n",
    "    from pyspark.sql.functions import year, month, hour, dayofmonth, dayofweek\n",
    "    df = layers[0]\n",
    "    df = df.filter(df['Parameter Name'] == 'PM2.5 - Local Conditions')\n",
    "#     df = df.filter(df.year == 2019)\n",
    "#     df = df.withColumn('dt', concat(col(\"Date Local\"), lit(\" \"), col(\"Time Local\")))\n",
    "    df = df.withColumn('date', F.unix_timestamp('Date GMT', 'yyyy-MM-dd').cast('timestamp'))\n",
    "    df = df.withColumn('month', month(df['date']))\n",
    "    df = df.withColumn('dayofmonth', dayofmonth(df['date']))\n",
    "    df = df.withColumn('dayofweek', dayofweek(df['date']))    \n",
    "    df.write.format(\"webgis\").save(\"date_parsed\" + str(dt.now().microsecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(code=date_parse, layers=[air_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df():\n",
    "    from datetime import datetime as dt\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    import pandas as pd\n",
    "    from fbprophet import Prophet\n",
    "    from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "    from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, FloatType, TimestampType, StringType\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    df1 = layers[0]\n",
    "    df1 = df1.withColumn('flag', lit(1))\n",
    "    schema = StructType([StructField('ds', TimestampType(), True),\n",
    "                        StructField('Outdoor_Temperature', FloatType(), True),\n",
    "                        StructField('Ozone', FloatType(), True),\n",
    "                        StructField('PM10_Total_0_10um_STP', FloatType(), True),\n",
    "                        StructField('y', FloatType(), True),\n",
    "                        StructField('Relative_Humidity', FloatType(), True),\n",
    "                        StructField('Wind_Direction___Resultant', FloatType(), True),\n",
    "                        StructField('Wind_Speed___Resultant', FloatType(), True),\n",
    "                        StructField('datetime', StringType(), True),\n",
    "                        StructField('flag', IntegerType(), True),\n",
    "                        StructField('year', IntegerType(), True)])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def forecast_pm25(df):\n",
    "        cols = ['Outdoor_Temperature', 'Ozone', 'PM10_Total_0_10um_STP',\n",
    "                'PM2_5___Local_Conditions', 'Relative_Humidity',\n",
    "                'Wind_Direction___Resultant',\n",
    "                'Wind_Speed___Resultant', 'datetime','flag']\n",
    "        df = df[cols]\n",
    "        df['Date'] = df['datetime'].astype('datetime64[ns]')\n",
    "        df['year'] = df['Date'].dt.year\n",
    "        df.set_index('Date', inplace=True) \n",
    "        df.sort_index(inplace=True)\n",
    "        v = pd.date_range(start='2016-12-31 23:00:00', periods=18265, freq='H', closed='right')\n",
    "        newdf = pd.DataFrame(index=v)\n",
    "\n",
    "        merge=pd.merge(newdf, df, how='left', left_index=True, right_index=True)\n",
    "        merge.interpolate(method='time', inplace=True)\n",
    "        merge.reset_index(inplace=True)\n",
    "        merge.rename(columns={'index': 'ds', 'PM2_5___Local_Conditions': 'y'}, inplace=True)\n",
    "        merge.interpolate('nearest', inplace=True)\n",
    "        merge['y'].fillna(0, inplace=True)\n",
    "        merge['PM10_Total_0_10um_STP'].fillna(0, inplace=True)\n",
    "        merge['Wind_Direction___Resultant'].fillna(0, inplace=True)\n",
    "        merge['Wind_Speed___Resultant'].fillna(0, inplace=True)\n",
    "        \n",
    "        for i,item in enumerate(merge['y']):\n",
    "            if item<=0:\n",
    "                merge['y'].iloc[i]=merge['y'].iloc[i-1]\n",
    "            else:\n",
    "                merge['y'].iloc[i]=item\n",
    "                \n",
    "        for i,item in enumerate(merge['PM10_Total_0_10um_STP']):\n",
    "            if item<=0:\n",
    "                merge['PM10_Total_0_10um_STP'].iloc[i]=merge['PM10_Total_0_10um_STP'].iloc[i-1]\n",
    "            else:\n",
    "                merge['PM10_Total_0_10um_STP'].iloc[i]=item        \n",
    "         \n",
    "        for i,item in enumerate(merge['Wind_Speed___Resultant']):\n",
    "            if item<=0:\n",
    "                merge['Wind_Speed___Resultant'].iloc[i]=merge['Wind_Speed___Resultant'].iloc[i-1]\n",
    "            else:\n",
    "                merge['Wind_Speed___Resultant'].iloc[i]=item\n",
    "        \n",
    "        for i,item in enumerate(merge['Wind_Direction___Resultant']):\n",
    "            if item<=0:\n",
    "                merge['Wind_Direction___Resultant'].iloc[i]=merge['Wind_Direction___Resultant'].iloc[i-1]\n",
    "            else:\n",
    "                merge['Wind_Direction___Resultant'].iloc[i]=item\n",
    "                \n",
    "        return merge[:10]\n",
    "    res = df1.groupby(['flag']).apply(forecast_pm25)\n",
    "#     res.toPandas().to_csv(r'\\\\DELDEVD014\\store\\forecast')\n",
    "    res.write.format(\"webgis\").save(\"merged_df_subset\" + str(dt.now().microsecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(code=process_df, layers=[series_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_item = gis.content.search('merged_df_subset')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_lyr = merge_item.tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate\n",
    "def fbprophet():\n",
    "    from datetime import datetime as dt\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    import pandas as pd\n",
    "    from fbprophet import Prophet\n",
    "    from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "    from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, FloatType, TimestampType\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    df1 = layers[0]\n",
    "    schema = StructType([StructField('ds', TimestampType(), True), \n",
    "                     StructField('yhat', FloatType(), True)],)\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def forecast_pm25(df):\n",
    "        train_df = merge[merge.year != 2019]\n",
    "        test_df = merge[merge.year == 2019] \n",
    "        test_df.drop(columns='y', inplace=True)     \n",
    "        \n",
    "        m = Prophet(daily_seasonality=True, weekly_seasonality=True)\n",
    "        m.fit(train_df);\n",
    "\n",
    "        forecast = (m.predict(test_df)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds = lambda x : pd.to_datetime(x[\"ds\"])))\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    res = df1.groupby(['flag']).apply(forecast_pm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python_script(code=process_df, layers=[series_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multivariate\n",
    "def fbprophet():\n",
    "    from datetime import datetime as dt\n",
    "    from pyspark.sql.functions import concat, col, lit\n",
    "    import pandas as pd\n",
    "    from fbprophet import Prophet\n",
    "    from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "    from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, FloatType, TimestampType\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    df1 = layers[0]\n",
    "    schema = StructType([StructField('ds', TimestampType(), True), \n",
    "                         StructField('trend', FloatType(), True),\n",
    "                         StructField('yhat_lower', FloatType(), True),\n",
    "                         StructField('yhat_upper', FloatType(), True),\n",
    "                         StructField('trend_lower', FloatType(), True),\n",
    "                         StructField('trend_upper', FloatType(), True),\n",
    "                         StructField('PM10_Total_0_10um_STP', FloatType(), True),\n",
    "                         StructField('PM10_Total_0_10um_STP_lower', FloatType(), True),\n",
    "                         StructField('PM10_Total_0_10um_STP_upper', FloatType(), True),\n",
    "                         StructField('Wind_Direction___Resultant', FloatType(), True),\n",
    "                         StructField('Wind_Direction___Resultant_lower', FloatType(), True),\n",
    "                         StructField('Wind_Direction___Resultant_upper', FloatType(), True),\n",
    "                         StructField('Wind_Speed___Resultant', FloatType(), True),\n",
    "                         StructField('Wind_Speed___Resultant_lower', FloatType(), True),\n",
    "                         StructField('Wind_Speed___Resultant_upper', FloatType(), True),\n",
    "                         StructField('additive_terms', FloatType(), True),\n",
    "                         StructField('additive_terms_lower', FloatType(), True),\n",
    "                         StructField('additive_terms_upper', FloatType(), True),\n",
    "                         StructField('daily', FloatType(), True),\n",
    "                         StructField('daily_lower', FloatType(), True),\n",
    "                         StructField('daily_upper', FloatType(), True),\n",
    "                         StructField('extra_regressors_additive', FloatType(), True),\n",
    "                         StructField('extra_regressors_additive_lower', FloatType(), True),\n",
    "                         StructField('extra_regressors_additive_upper', FloatType(), True),\n",
    "                         StructField('weekly', FloatType(), True),\n",
    "                         StructField('weekly_lower', FloatType(), True),\n",
    "                         StructField('weekly_upper', FloatType(), True),\n",
    "                         StructField('multiplicative_terms', FloatType(), True),\n",
    "                         StructField('multiplicative_terms_lower', FloatType(), True),\n",
    "                         StructField('multiplicative_terms_upper', FloatType(), True),\n",
    "                         StructField('yhat', FloatType(), True)])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def forecast_pm25(df):           \n",
    "        train_df = merge[merge.year != 2019]\n",
    "        test_df = merge[merge.year == 2019]\n",
    "        test_df.drop(columns='y', inplace=True)        \n",
    "            \n",
    "        m = Prophet(daily_seasonality=True,\n",
    "                    weekly_seasonality=True)\n",
    "        m.add_regressor('PM10_Total_0_10um_STP')\n",
    "        m.add_regressor('Wind_Speed___Resultant')\n",
    "        m.add_regressor('Wind_Direction___Resultant')\n",
    "        m.fit(train_df);\n",
    "        m.save(r'\\\\DELDEVD014\\store\\forecast_model')\n",
    "        forecast = m.predict(test_df)\n",
    "        return forecast\n",
    "    res = df1.groupby(['flag']).apply(forecast_pm25)\n",
    "    res.write.format(\"webgis\").save(\"predicted_pm25\" + str(dt.now().microsecond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
